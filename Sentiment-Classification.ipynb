{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOUKIp9VC0gQ"
   },
   "source": [
    "# CT5146 Assignment 2\n",
    "\n",
    "This task is focused on sentiment classification and span extraction from tweets. Please complete this task and upload your answers to Canvas as an iPython Notebook or a PDF. This assignment is due by 23:59 on December 10th 2023. Late submissions will be penalised by 1% for each day after this date. This is an individual assignment and your work must be your own.\n",
    "\n",
    "You may use libraries such as SciKit-Learn to complete this assignment, however you should justify the choice of functions from these libraries.\n",
    "\n",
    "**Summary**\n",
    "\n",
    "In this task, given a tweet (text) we have two objectives:\n",
    "\n",
    "1. Sentiment classification: Classify the tweet into one of three classes (positive, negative, neutral).\n",
    "\n",
    "2. Sentiment span extraction: Extract the sequence of words from the tweet that expresses the given sentiment.\n",
    "\n",
    "Consider the following example from the train dataset:\n",
    "\n",
    "| textID | text | selected_text | sentiment |\n",
    "| ------ | ---- | ------------- | --------- |\n",
    "| 266b8792a0 |Just broke my favorite necklace  superglue? | Just broke my favorite necklace | negative |\n",
    "| 8f3e73cf09 | \"Screw the reviews, I thought Wolverine was awesome. But not enough Dominic Monaghan for my liking.\" | I thought Wolverine was awesome. | positive |\n",
    "|... | ... | ... | ... |\n",
    "| 266b8792a0 |Just broke my favorite necklace  superglue? | Just broke my favorite necklace | negative |\n",
    "\n",
    "\n",
    "The dataset is divided into `train`, `dev` and `test` sets. The `train` set is used for the model training, while the `dev` set is used for validation and hyperparameter tuning.\n",
    "\n",
    "The test dataset has **no sentiment labels**. This dataset split will be used for **leaderboard submission** for sentiment classification modelling, described in section 2.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bS4AHgRYC0gT"
   },
   "source": [
    "# 1. Data Analysis\n",
    "    \n",
    "## Task 1a\n",
    "\n",
    "Plot a group bar plot to show the distribution of sentiment classes (positive, negative, neutral) in the train and dev dataset. As shown in the following illustration. `(10 marks)`\n",
    "    \n",
    "    \n",
    "   <img src=\"https://github.com/gauneg/gauneg.github.io/blob/main/ds1.jpg?raw=true\" alt=\"PLOT EXAMPLE\" width=\"400px\"/>\n",
    "\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1A)Bar Chart\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Pull in data from train/dev files and distinguish\n",
    "train_csv = 'C:/Users/Cathal/NLP_A2_Datasets/train.csv'\n",
    "dev_csv = 'C:/Users/Cathal/NLP_A2_Datasets/dev.csv'\n",
    "train_df = pd.read_csv(train_csv)\n",
    "dev_df = pd.read_csv(dev_csv)\n",
    "\n",
    "train_df['dataset'] = 'train'\n",
    "dev_df['dataset'] = 'dev'\n",
    "\n",
    "#Convat df's\n",
    "concat_data = pd.concat([train_df, dev_df])\n",
    "\n",
    "#Count frequency\n",
    "sentiment_freq = concat_data.groupby(['sentiment', 'dataset']).size().unstack()\n",
    "\n",
    "#Plot chart\n",
    "barchart = sentiment_freq.plot(kind='bar')\n",
    "\n",
    "for x in barchart.patches:\n",
    "    barchart.annotate(str(x.get_height()), (x.get_x() + x.get_width() / 2., x.get_height()), xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "#Add additional details for the bar chart - legend, xlabel, etc.\n",
    "plt.title('Sentiment Class Distribution')\n",
    "plt.legend(title='Datasets', loc='upper left')\n",
    "plt.xlabel('Sentiment Type')\n",
    "plt.ylabel('Occurrences')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZdmAEjvC0gT"
   },
   "source": [
    "# 2. Sentiment Classification\n",
    "\n",
    "## Task 2a\n",
    "\n",
    "Train a sequence classification model. The model should take the `text` as input and return one out of 3 sentiment classes: `negative`, `neutral` or `positive`.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/gauneg/gauneg.github.io/blob/main/sentiment_classification.png?raw=true\" alt=\"PLOT EXAMPLE\" width=\"500px\"/>\n",
    "\n",
    "    \n",
    "INPUT : (`text`)\n",
    "\n",
    "OUTPUT: `sentiment_class` i.e. one class from `(positive, negative, neutral)`\n",
    "\n",
    "\n",
    "* You should preprocess the data in the training and development sets and extract feature vectors using either bag-of-words or TF-IDF. `(15 marks)`\n",
    "* Using these features please train a classifier using a method such Support Vector Machines (SVM) `(10 marks)`\n",
    "\n",
    "Please provide an  implementation along with an **explanation** (no more than 5 0 words) of the method used and any libraries you used to do this.\n",
    "\n",
    "## Task 2b\n",
    "\n",
    "Use the `text` and `sentiment` of the dev set to calulate the efficiency of your model by calculating the following metrics for each class:\n",
    "    \n",
    "- Precision\n",
    "\n",
    "- Recall\n",
    "\n",
    "- F1 Score\n",
    "    \n",
    "Considering it is a multi-class classification task, you should also report the macro-average scores in your implementation of the automatic metrics.\n",
    "Please provide an implementation along with an **explanation** (no more than 50 words) of the method used. `(5 marks)`\n",
    "\n",
    "\n",
    "## Task 2c\n",
    "\n",
    "Based on the methods discussed in the lecture suggest **ONE** improvement or alternative approach that can be applied to the sentiment classification task as in Task 2a. This may be through better feature extraction, new modelling or through an alternaitve methodology.\n",
    "\n",
    "You should implement this approach and compare the results using the evaluation from Task 2b.\n",
    "\n",
    "Provide an **explanation** of your approach in no more than 100 words.\n",
    "\n",
    "`(20 marks)`\n",
    "\n",
    "## Task 2d\n",
    "Join the closed kaggle competition at this [link](https://www.kaggle.com/t/c485f589d0694836be2dcecd01a7da4a). Follow the instructions. In order to join the competion login to kaggle using your `universityofgalway` email account. Make a successful submission. Apply the system you developed in Task 2c to the test dataset and submit this result. Marks will be awarded based on the quality of the result.\n",
    "\n",
    "`(10 marks)`\n",
    "\n",
    "Format of the test set:\n",
    "\n",
    "\n",
    "| textID | text | selected_text |\n",
    "| ------ | ---- | ------------- |\n",
    "| 266b8792a0 |Just broke my favorite necklace  superglue? | Just broke my favorite necklace |\n",
    "| 8f3e73cf09 | \"Screw the reviews, I thought Wolverine was awesome. But not enough Dominic Monaghan for my liking.\" | I thought Wolverine was awesome. |\n",
    "|... | ... | ... |\n",
    "| 266b8792a0 |Just broke my favorite necklace  superglue? | Just broke my favorite necklace |\n",
    "\n",
    "Format of the submission file `(.csv format)`:\n",
    "\n",
    "\n",
    "| textID | sentiment |\n",
    "| ------ | --------- |\n",
    "| 266b8792a0 | negative |\n",
    "| 8f3e73cf09 | positive |\n",
    "|... | ... |\n",
    "| 266b8792a0 | negative |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6638926366472899\n",
      "Recall: 0.6607142857142857\n",
      "F-Score: 0.6605435986801442\n"
     ]
    }
   ],
   "source": [
    "#MY ORIGINAL APPRAOCH USES VARIOUS DATA PREPROCESSING STEPS. FEATURE EXTRACTION IS PERFORMED BY TF-IDF METHOD.\n",
    "#MODEL USED - SVM (SVC)\n",
    "#PREDICTIONS ARE THEN MADE BASED OFF OF THE TRAIN (TO TRAIN) AND DEV (TO VALIDATE).\n",
    "#VARIOUS METRICS - PRECISION, RECALL, F-MEASURE ARE USED TO MEASURE THE MODELS PERFORMANCE\n",
    "\n",
    "\n",
    "#2A + B) PRE-PROCESSING OF TRAIN AND DEV DATASETS\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "#SPLIT 'TEXT' COLUMNS\n",
    "df_train_text = train_df[\"text\"]\n",
    "df_dev_text = dev_df[\"text\"]\n",
    "\n",
    "#TOKENIZATION\n",
    "train_tokenization = df_train_text.apply(lambda x: word_tokenize(str(x)))\n",
    "dev_tokenization = df_dev_text.apply(lambda x: word_tokenize(str(x)))\n",
    "\n",
    "#CONVERT TO LOWERCASE\n",
    "train_lowercase = train_tokenization.apply(lambda x: [word.lower() for word in x])\n",
    "dev_lowercase = dev_tokenization.apply(lambda x: [word.lower() for word in x])\n",
    "\n",
    "#STOPWORD REMOVAL\n",
    "stopwords = set(stopwords.words('english'))\n",
    "train_less_stopwords = train_lowercase.apply(lambda x: [word for word in x if word not in stopwords])\n",
    "dev_less_stopwords = dev_lowercase.apply(lambda x: [word for word in x if word not in stopwords])\n",
    "\n",
    "#SPECIAL CHARACTER REMOVAL \n",
    "special_characters = string.punctuation\n",
    "train_cleaned = train_less_stopwords.apply(lambda x: [word.translate(str.maketrans(\"\", \"\", special_characters)) for word in x])\n",
    "dev_cleaned = dev_less_stopwords.apply(lambda x: [word.translate(str.maketrans(\"\", \"\", special_characters)) for word in x])\n",
    "\n",
    "#BLANK SPACE REMOVAL \n",
    "train_space_removal = train_cleaned.apply(lambda x: [space for space in x if space != ''])\n",
    "dev_space_removal = dev_cleaned.apply(lambda x: [space for space in x if space != ''])\n",
    "\n",
    "#LEMMATIZATION\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "train_lemmatized = train_space_removal.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "dev_lemmatized = dev_space_removal.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "#2A)FEATURE EXTRACTION - TF-IDF\n",
    "train_join = train_lemmatized.apply(' '.join)\n",
    "dev_join = dev_lemmatized.apply(' '.join)\n",
    "\n",
    "#TF-IDF Approach\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=750, token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "train_tfidf_vectorize = tfidf_vectorizer.fit_transform(train_join)\n",
    "dev_tfidf_vectorize = tfidf_vectorizer.transform(dev_join)\n",
    "\n",
    "#Convert to DataFrames for better readability\n",
    "train_tf_idf = pd.DataFrame(train_tfidf_vectorize.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "dev_tf_idf = pd.DataFrame(dev_tfidf_vectorize.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "#2A)Classifier - SVM\n",
    "svm_classifier = SVC(C=0.7, kernel='rbf', random_state=42, class_weight='balanced')\n",
    "\n",
    "train_sentiment = train_df[\"sentiment\"]\n",
    "dev_sentiment = dev_df[\"sentiment\"]\n",
    "\n",
    "#Train SVM on train dataset\n",
    "svm_classifier.fit(train_tf_idf, train_sentiment)\n",
    "\n",
    "#Predict on dev dataset\n",
    "dev_predictions_tf_idf = svm_classifier.predict(dev_tf_idf)\n",
    "\n",
    "#2B)EVALUATION METRICS\n",
    "#Precision\n",
    "precision_tf_idf = precision_score(dev_sentiment, dev_predictions_tf_idf, average='weighted')\n",
    "print(\"Precision:\", precision_tf_idf)\n",
    "\n",
    "#Recall\n",
    "recall_tf_idf = recall_score(dev_sentiment, dev_predictions_tf_idf, average='weighted')\n",
    "print(\"Recall:\", recall_tf_idf)\n",
    "\n",
    "#F-Score\n",
    "f1_tf_idf = f1_score(dev_sentiment, dev_predictions_tf_idf, average='weighted')\n",
    "print(\"F-Score:\", f1_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision BOW 0.6769436130385006\n",
      "RecalL BOW 0.6741071428571429\n",
      "F-Score BOW 0.6725100330411757\n",
      "=================================\n",
      "Precision TF-IDF 0.6638926366472899\n",
      "RecalL TF-IDF 0.6607142857142857\n",
      "F-Score TF-IDF 0.6605435986801442\n"
     ]
    }
   ],
   "source": [
    "#MY UPDATED APPRAOCH USES VARIOUS DATA PREPROCESSING STEPS. FEATURE EXTRACTION IS PERFORMED BY BAG OF WORDS METHOD.\n",
    "#MODEL USED - SVM (SVC)\n",
    "#PREDICTIONS ARE THEN MADE BASED OFF OF THE TRAIN (TO TRAIN) AND DEV (TO VALIDATE).\n",
    "#VARIOUS METRICS - PRECISION, RECALL, F-MEASURE ARE USED TO MEASURE THE MODELS PERFORMANCE\n",
    "#BAG OF WORDS IMPROVED THE ACCURACY OF MY METHOD AS OPPOSED TO USING MY ORIGINAL TF-IDF APPRACH (+5% BETTER SCORES)\n",
    "\n",
    "#2C)FEATURE EXTRACTION - Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train_join = train_lemmatized.apply(' '.join)\n",
    "dev_join = dev_lemmatized.apply(' '.join)\n",
    "\n",
    "#Bag of Words Approach\n",
    "count_vectorizer = CountVectorizer(max_features=750, token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "train_bag_vectorize = count_vectorizer.fit_transform(train_join)\n",
    "dev_bag_vectorize = count_vectorizer.transform(dev_join)\n",
    "\n",
    "# Convert to DataFrames for better readability\n",
    "train_bow = pd.DataFrame(train_bag_vectorize.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "dev_bow = pd.DataFrame(dev_bag_vectorize.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "svm_classifier = SVC(C=0.7, kernel='rbf', random_state=42, class_weight='balanced')\n",
    "#Train SVM on train dataset\n",
    "svm_classifier.fit(train_bow, train_sentiment)\n",
    "\n",
    "#Predict on dev dataset\n",
    "dev_predictions_bow = svm_classifier.predict(dev_bow)\n",
    "\n",
    "#Metrics - Precision, Recall, F-Score\n",
    "precision_bow = precision_score(dev_sentiment, dev_predictions_bow, average='weighted')\n",
    "\n",
    "recall_bow = recall_score(dev_sentiment, dev_predictions_bow, average='weighted')\n",
    "\n",
    "f1_bow = f1_score(dev_sentiment, dev_predictions_bow, average='weighted')\n",
    "\n",
    "print(\"Precision BOW\", precision_bow)\n",
    "print(\"RecalL BOW\", recall_bow)\n",
    "print(\"F-Score BOW\", f1_bow)\n",
    "print(\"=================================\")\n",
    "print(\"Precision TF-IDF\", precision_tf_idf)\n",
    "print(\"RecalL TF-IDF\", recall_tf_idf)\n",
    "print(\"F-Score TF-IDF\", f1_tf_idf)\n",
    "\n",
    "#2A)Classifier - SVM\n",
    "# Create an instance of the SVM classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_classifier = SVC(C=0.7, kernel='linear', class_weight='balanced')\n",
    "\n",
    "train_sentiment = train_df[\"sentiment\"]\n",
    "dev_sentiment = dev_df[\"sentiment\"]\n",
    "\n",
    "#Train SVM on train dataset\n",
    "svm_classifier.fit(train_tf_idf, train_sentiment)\n",
    "\n",
    "#Predict on dev dataset\n",
    "dev_predictions_tf_idf = svm_classifier.predict(dev_tf_idf)\n",
    "\n",
    "#2A)Classifier - SVM\n",
    "# Create an instance of the SVM classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_classifier = SVC(C=0.7, kernel='linear', class_weight='balanced')\n",
    "\n",
    "train_sentiment = train_df[\"sentiment\"]\n",
    "dev_sentiment = dev_df[\"sentiment\"]\n",
    "\n",
    "#Train SVM on train dataset\n",
    "svm_classifier.fit(train_tf_idf, train_sentiment)\n",
    "\n",
    "#Predict on dev dataset\n",
    "dev_predictions_tf_idf = svm_classifier.predict(dev_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2D - KAGGLE\n",
    "test_csv = 'C:/Users/Cathal/NLP_A2_Datasets/test.csv'\n",
    "test_df = pd.read_csv(test_csv)\n",
    "\n",
    "df_test_text = test_df[\"text\"]\n",
    "\n",
    "#TOKENIZATION\n",
    "test_tokenization = df_test_text.apply(lambda x: word_tokenize(str(x)))\n",
    "\n",
    "#CONVERT TO LOWERCASE\n",
    "test_lowercase = test_tokenization.apply(lambda x: [word.lower() for word in x])\n",
    "\n",
    "#STOPWORD REMOVAL\n",
    "test_less_stopwords = test_lowercase.apply(lambda x: [word for word in x if word not in stopwords])\n",
    "\n",
    "#SPECIAL CHARACTER REMOVAL \n",
    "test_cleaned = test_less_stopwords.apply(lambda x: [word.translate(str.maketrans(\"\", \"\", special_characters)) for word in x])\n",
    "\n",
    "#BLANK SPACE REMOVAL \n",
    "test_space_removal = test_cleaned.apply(lambda x: [space for space in x if space != ''])\n",
    "\n",
    "#LEMMATIZATION\n",
    "test_lemmatized = test_space_removal.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "test_join = test_lemmatized.apply(' '.join)\n",
    "\n",
    "#Bag of Words Approach\n",
    "count_vectorizer = CountVectorizer(max_features=750, token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "test_bag_vectorize = count_vectorizer.fit_transform(test_join)\n",
    "\n",
    "# Convert to DataFrames for better readability\n",
    "test_bow = pd.DataFrame(test_bag_vectorize.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "svm_classifier = SVC(C=0.7, kernel='rbf', random_state=42, class_weight='balanced')\n",
    "#Train SVM on train dataset\n",
    "svm_classifier.fit(train_bow, train_sentiment)\n",
    "\n",
    "#Predict on dev dataset\n",
    "dev_predictions_bow = svm_classifier.predict(test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwpy2c2OC0gT"
   },
   "source": [
    "# 3. Span Extraction\n",
    "## Task 3a\n",
    "\n",
    "\n",
    "Our goal for this task is to extract the sentiment span, given the text and the given sentiment (true sentiment label) as input. The sentiment span is a subsection of the text that expresses the sentiment that classifies the text overall.\n",
    "\n",
    "INPUT : (`text`, `sentiment`)\n",
    "\n",
    "OUTPUT: `selected_text`\n",
    "\n",
    "<img src=\"https://github.com/gauneg/gauneg.github.io/blob/main/assignment_diag.png?raw=true\" alt=\"PLOT EXAMPLE\" width=\"500px\"/>\n",
    "\n",
    "\n",
    "1. Describe in no more than 200 words a system that could be used to identify the sentiment span. You should consider the methodology potentially including how features are extracted, what models could be used and what procedures should be used to train the model. `(10 marks)`\n",
    "\n",
    "2. Implement this system and apply it to the train and development splits of the dataset. `(15 marks)`\n",
    "\n",
    "## Task 3b\n",
    "\n",
    "1. Describe an automatic metric that can be used to evaluate the task of span extraction. Implement this metric and use it to evaluate the performance of the system you developed in Task 3a. `(5 marks)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MY APPRAOCH WAS TO USE THE SAME DATA PREPROCESSING STEPS. FEATURE EXTRACTION IS PERFORMED BY BAG OF WORDS METHOD AGAIN.\n",
    "#MODEL USED - SVM (SVC) TO BE USED AGAIN\n",
    "#MY AIM WAS TO COMBINE THE 2 FEATURES (TEXT, SNETIMENT) IN ORDER TO HAVE 1 COMBINED FEATURE OF THESE\n",
    "#I RAN INTO AN ERROR AS MY NUMBER OF FEATURES DID NOT MATCH UP (I BELIEVE SENTIMENT HAD MORE IN IT)\n",
    "#THIS WOULD BE DUE TO THE FACT THAT IT DIDN'T GO THROUGH AS MUCH DATA PRE-PROCESSING WHICH REMOVED SOME VALUES FOR TEXT\n",
    "#PREDICTIONS WOULD THEN BE MADE BASED OFF OF THE TRAIN (TO TRAIN) AND DEV (TO VALIDATE).\n",
    "#VARIOUS METRICS - PRECISION, RECALL, F-MEASURE ARE USED TO MEASURE THE MODELS PERFORMANCE\n",
    "\n",
    "#3A + B)SENTIMENT SPAN \n",
    "#Inputs 'text' (df_train_text + df_dev_text) and 'sentiment' (train_sentiment + dev_sentiment) \n",
    "#Output 'selected_text' (df_train_selected_text + df_dev_selected_text)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "df_train_selected_text = train_df[\"selected_text\"]\n",
    "df_dev_selected_text = dev_df[\"selected_text\"]\n",
    "\n",
    "#TOKENIZATION\n",
    "train_tokenization_sentiment = train_sentiment.apply(lambda x: word_tokenize(str(x)))\n",
    "dev_tokenization_sentiment = dev_sentiment.apply(lambda x: word_tokenize(str(x)))\n",
    "\n",
    "#CONVERT TO LOWERCASE\n",
    "train_lowercase_sentiment = train_tokenization_sentiment.apply(lambda x: [word.lower() for word in x])\n",
    "dev_lowercase_sentiment = dev_tokenization_sentiment.apply(lambda x: [word.lower() for word in x])\n",
    "\n",
    "#STOPWORD REMOVAL\n",
    "train_less_stopwords_sentiment = train_lowercase_sentiment.apply(lambda x: [word for word in x if word not in stop_words])\n",
    "dev_less_stopwords_sentiment = dev_lowercase_sentiment.apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "#SPECIAL CHARACTER REMOVAL \n",
    "train_cleaned_sentiment = train_less_stopwords_sentiment.apply(lambda x: [word.translate(str.maketrans(\"\", \"\", special_characters)) for word in x])\n",
    "dev_cleaned_sentiment = dev_less_stopwords_sentiment.apply(lambda x: [word.translate(str.maketrans(\"\", \"\", special_characters)) for word in x])\n",
    "\n",
    "#LEMMATIZATION\n",
    "train_lemmatized_sentiment = train_cleaned_sentiment.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "dev_lemmatized_sentiment = dev_cleaned_sentiment.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "#Join\n",
    "train_join_sentiment = train_lemmatized_sentiment.apply(' '.join)\n",
    "dev_join_sentiment = dev_lemmatized_sentiment.apply(' '.join)\n",
    "\n",
    "#Combine the two text inputs in the training set\n",
    "train_concat = pd.concat([train_join_selected, train_join_sentiment], axis=1)\n",
    "\n",
    "#Combine the two text inputs in the testing set\n",
    "dev_concat = pd.concat([dev_join_selected, dev_join_sentiment], axis=1)\n",
    "\n",
    "#Bag of Words Approach\n",
    "count_vectorizer = CountVectorizer(max_features=1000, token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "train_bag_vectorize_sentiment = count_vectorizer.fit_transform(train_concat)\n",
    "dev_bag_vectorize_sentiment = count_vectorizer.transform(dev_concat)\n",
    "\n",
    "# Convert to DataFrames for better readability\n",
    "train_bow_sentiment = pd.DataFrame(train_bag_vectorize_sentiment.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "dev_bow_sentiment = pd.DataFrame(dev_bag_vectorize_sentiment.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "svm_classifier = SVC(C=1.0, kernel='linear', random_state=42, class_weight='balanced')\n",
    "\n",
    "#Train SVM on train dataset\n",
    "svm_classifier.fit(train_bag_vectorize_sentiment, df_train_selected_text)\n",
    "\n",
    "#Predict on train dataset\n",
    "dev_predictions_bow = svm_classifier.predict(dev_bow_sentiment)\n",
    "\n",
    "#Metrics - Precision, Recall, F-Score\n",
    "precision_bow_3B = precision_score(dev_predictions_bow, df_dev_selected_text, average='weighted')\n",
    "recall_bow__3B = recall_score(dev_predictions_bow, df_dev_selected_text, average='weighted')\n",
    "f1_bow__3B = f1_score(dev_predictions_bow, df_dev_selected_text, average='weighted')\n",
    "\n",
    "print(\"Precision BOW\", precision_bow_3B)\n",
    "print(\"RecalL BOW\", recall_bow__3B)\n",
    "print(\"F-Score BOW\", f1_bow__3B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
