{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f472157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART2 - Basic Parameters\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#Variables\n",
    "BOARD_ROWS = 5 #Number of rows \n",
    "BOARD_COLS = 5 #Number of columns\n",
    "WIN_STATE = (4, 4) #Position of the win state - bottom right corner\n",
    "HOLES = [(1, 0), (1, 3), (3, 1), (4, 2)] #Position of the holes\n",
    "START = (0, 0) #Starting position of the agent\n",
    "\n",
    "DEBUG = False  # set to true to enable verbose output\n",
    "\n",
    "\n",
    "class State:\n",
    "    def __init__(self, state=START):        \n",
    "        self.state = state\n",
    "        self.isEnd = False        \n",
    "\n",
    "    #Returns a reward of 10 if end state is reaches\n",
    "    #A reward of -5 if a hole is fallen into\n",
    "    #Or a reward of -1 if any other movement besides those \n",
    "    def get_reward(self):\n",
    "        if self.state == WIN_STATE:\n",
    "            return 10.0        \n",
    "        elif self.state in HOLES:\n",
    "            return -5.0\n",
    "        else:\n",
    "            return -1.0\n",
    "\n",
    "        \n",
    "    #Checks if an end state is reached - either falling into a hold or reaching the end state, if so, episode is ended   \n",
    "    def is_end_func(self):\n",
    "        if (self.state == WIN_STATE) or (self.state in HOLES):\n",
    "            self.isEnd = True\n",
    "\n",
    "            \n",
    "    #Responsible for moving the agent in one of 4 directions - up,down,left or right\n",
    "    def nxt_position(self, action):\n",
    "        if action == 0:                \n",
    "            nxt_state = (self.state[0] - 1, self.state[1]) #0 - moves agent up\n",
    "        elif action == 1:\n",
    "            nxt_state = (self.state[0] + 1, self.state[1]) #1 - moves agent down\n",
    "        elif action == 2:\n",
    "            nxt_state = (self.state[0], self.state[1] - 1) #2 - moves agent left\n",
    "        else:\n",
    "            nxt_state = (self.state[0], self.state[1] + 1) #3 - moves agent right\n",
    "            \n",
    "        #Makes sure that the movement is valid within the square, if its not, agent remains in the same position      \n",
    "        if (nxt_state[0] >= 0) and (nxt_state[0] < BOARD_ROWS):\n",
    "            if (nxt_state[1] >= 0) and (nxt_state[1] < BOARD_COLS):\n",
    "                return nxt_state  \n",
    "        return self.state  \n",
    "\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = [0, 1, 2, 3]  # up, down, left, right\n",
    "        self.State = State()\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.5\n",
    "        self.eps = 0.1\n",
    "        self.episode_rewards = []  \n",
    "        \n",
    "       \n",
    "        self.action_values = {}        \n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                for k in range(len(self.actions)):\n",
    "                    self.action_values[(i, j, k)] = 0.0 \n",
    "        \n",
    "        self.new_action_values = []\n",
    "    \n",
    "    #Selects an action \n",
    "    def choose_action(self, current_state):\n",
    "        # choose action according to policy eps-greedy\n",
    "        if np.random.uniform(0, 1) <= self.eps:\n",
    "            action = np.random.choice(self.actions)\n",
    "            if DEBUG:\n",
    "                print(\"selecting random action\")\n",
    "        else:\n",
    "            action = self.best_action(current_state)\n",
    "        return action\n",
    "\n",
    "    #executes the selected action and updates the current state of the agent\n",
    "    def take_action(self, action):\n",
    "        position = self.State.nxt_position(action)\n",
    "        self.State.state = position\n",
    "\n",
    "    #Shows the best action for a state based on Q-Learning method    \n",
    "    def best_action(self, state):\n",
    "        best = -1\n",
    "        max_val = -100000000\n",
    "        for a in self.actions:\n",
    "            q_val = self.action_values[state[0], state[1], a]\n",
    "            if q_val >= max_val:\n",
    "                max_val = q_val\n",
    "                best = a\n",
    "        return best\n",
    "\n",
    "    #Calculates max q-value for a state\n",
    "    def q_max(self, state):\n",
    "        best = self.best_action(state)\n",
    "        return self.action_values[state[0], state[1], best]\n",
    "\n",
    "    #Implements q-learning\n",
    "    def q_learning(self, episodes):       \n",
    "        #Q-learning implementation\n",
    "        x = 0  \n",
    "        while x < episodes:\n",
    "            # Init S\n",
    "            self.State.isEnd = False\n",
    "            self.State.state = START  # Re init S Start state\n",
    "            step = 0\n",
    "            total_reward = 0  #Total reward per episode\n",
    "\n",
    "            if DEBUG:\n",
    "                print(\"**** Beginning episode\", x, \"****\")\n",
    "                self.show_values()\n",
    "\n",
    "            while True:  # repeat for each step of the episode (until S is terminal)\n",
    "\n",
    "                # Store current state for Q update\n",
    "                current_state = (self.State.state[0], self.State.state[1])\n",
    "\n",
    "                # Choose action A from S using policy derived from Q (e-greedy)\n",
    "                action = self.choose_action(current_state)\n",
    "\n",
    "                # Take action A observe R and next State S'\n",
    "                self.take_action(action)\n",
    "                reward = self.State.get_reward()\n",
    "                total_reward += reward  # Accumulate reward for this episode\n",
    "                self.State.is_end_func()\n",
    "                next_state = self.State.state[0], self.State.state[1]\n",
    "\n",
    "                # Update state action values\n",
    "                old_q = self.action_values[current_state[0], current_state[1], action]\n",
    "                max_q = self.q_max(next_state)\n",
    "                new_q = old_q + self.alpha * (reward + self.gamma * max_q - old_q)\n",
    "                self.action_values[current_state[0], current_state[1], action] = new_q\n",
    "\n",
    "                step += 1\n",
    "                if DEBUG:\n",
    "                    print(\"step\", step, \"state\", current_state, \"action\", action, \"reward\", reward,\n",
    "                          \"next_state\", next_state, \"old_q\", old_q, \"max_q\", max_q, \"new_q\", new_q)\n",
    "\n",
    "                # Check if s is terminal\n",
    "                if self.State.isEnd:\n",
    "                    self.episode_rewards.append(total_reward)  #Append total reward for episode\n",
    "                    break\n",
    "\n",
    "                # S <- S' automatically when I took the action                    \n",
    "\n",
    "            x += 1           \n",
    "            \n",
    "    #Displays the actions in a square format\n",
    "    def show_values(self):\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('-----------------------------------------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                mx_nxt_value = -1000\n",
    "                for a in self.actions:\n",
    "                    nxt_value = self.action_values[(i, j, a)]\n",
    "                    if nxt_value >= mx_nxt_value:\n",
    "                        mx_nxt_value = nxt_value                \n",
    "                out += str(round(mx_nxt_value, 3)).ljust(6) + ' | '\n",
    "            print(out)\n",
    "        print('-----------------------------------------------')\n",
    "    \n",
    "    \n",
    "    #Outputs the max action value estimate for each state to a (.txt) file\n",
    "    def max_action_value_output(self, filename):\n",
    "            with open(filename, 'w') as f:\n",
    "                for i in range(BOARD_ROWS):\n",
    "                    for j in range(BOARD_COLS):\n",
    "                        max_value = -float('inf')\n",
    "                        for a in self.actions:\n",
    "                            current_value = self.action_values[(i, j, a)]\n",
    "                            if current_value > max_value:\n",
    "                                max_value = current_value\n",
    "                        f.write(f\"State ({i}, {j}): {max_value}\\n\")    \n",
    "                        \n",
    "                        \n",
    "   \n",
    "\n",
    "                        \n",
    "    \n",
    "    def plot_rewards_per_episode(self):\n",
    "        episode_numbers = range(1, len(self.episode_rewards) + 1)\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.plot(episode_numbers, self.episode_rewards, color='black')\n",
    "        max_reward = max(self.episode_rewards)\n",
    "        plt.axhline(y=max_reward, color='red', linewidth=3, linestyle='--', label=f'Max Reward: {max_reward}')\n",
    "        avg_reward = self.average_reward()\n",
    "        plt.axhline(y=avg_reward, color='green', linewidth=3, linestyle='--', label=f'Avg Reward: {avg_reward}')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.title('Reward per Episode')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "      \n",
    "        \n",
    "    def average_reward(self):\n",
    "        return np.mean(self.episode_rewards) if self.episode_rewards else 0.0\n",
    "        \n",
    "         \n",
    "        \n",
    "#Creates an agent, implements q-learning into it for a specified number of episodes, shows the\n",
    "#results and outputs max action values to .txt file       \n",
    "if __name__ == \"__main__\":\n",
    "    ag = Agent()\n",
    "    ag.q_learning(10000)\n",
    "    ag.show_values()\n",
    "    ag.max_action_value_output('C:/Users/Cathal/OneDrive/Masters/2nd Year/Sem 2/Agents, Multi-Agent Systems and Reinforcement Learning/Assignments/Assignment 2/Max Action Values (.txt)/Original_Parameters.txt')  #Save values to file\n",
    "    ag.plot_rewards_per_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dded14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART2 - Epsilon Decay Method with same parameters as previous method \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#Variables\n",
    "BOARD_ROWS = 5 #Number of rows \n",
    "BOARD_COLS = 5 #Number of columns\n",
    "WIN_STATE = (4, 4) #Position of the win state - bottom right corner\n",
    "HOLES = [(1, 0), (1, 3), (3, 1), (4, 2)] #Position of the holes\n",
    "START = (0, 0) #Starting position of the agent\n",
    "\n",
    "DEBUG = False  # set to true to enable verbose output\n",
    "\n",
    "\n",
    "class State:\n",
    "    def __init__(self, state=START):        \n",
    "        self.state = state\n",
    "        self.isEnd = False        \n",
    "\n",
    "    #Returns a reward of 10 if end state is reaches\n",
    "    #A reward of -5 if a hole is fallen into\n",
    "    #Or a reward of -1 if any other movement besides those \n",
    "    def get_reward(self):\n",
    "        if self.state == WIN_STATE:\n",
    "            return 10.0        \n",
    "        elif self.state in HOLES:\n",
    "            return -5.0\n",
    "        else:\n",
    "            return -1.0\n",
    "\n",
    "        \n",
    "    #Checks if an end state is reached - either falling into a hold or reaching the end state, if so, episode is ended   \n",
    "    def is_end_func(self):\n",
    "        if (self.state == WIN_STATE) or (self.state in HOLES):\n",
    "            self.isEnd = True\n",
    "\n",
    "            \n",
    "    #Responsible for moving the agent in one of 4 directions - up,down,left or right\n",
    "    def nxt_position(self, action):\n",
    "        if action == 0:                \n",
    "            nxt_state = (self.state[0] - 1, self.state[1]) #0 - moves agent up\n",
    "        elif action == 1:\n",
    "            nxt_state = (self.state[0] + 1, self.state[1]) #1 - moves agent down\n",
    "        elif action == 2:\n",
    "            nxt_state = (self.state[0], self.state[1] - 1) #2 - moves agent left\n",
    "        else:\n",
    "            nxt_state = (self.state[0], self.state[1] + 1) #3 - moves agent right\n",
    "            \n",
    "        #Makes sure that the movement is valid within the square, if its not, agent remains in the same position      \n",
    "        if (nxt_state[0] >= 0) and (nxt_state[0] < BOARD_ROWS):\n",
    "            if (nxt_state[1] >= 0) and (nxt_state[1] < BOARD_COLS):\n",
    "                return nxt_state  \n",
    "        return self.state  \n",
    "\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = [0, 1, 2, 3]  # up, down, left, right\n",
    "        self.State = State()\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.5\n",
    "        self.eps = 1.0  #Initial value of epsilon\n",
    "        self.min_eps = 0.0  #Minimum value of eps\n",
    "        self.eps_decay = 0.0001  #Decay rate \n",
    "        self.episode_rewards = []  \n",
    "        \n",
    "        # initialise state values\n",
    "        self.action_values = {}        \n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                for k in range(len(self.actions)):\n",
    "                    self.action_values[(i, j, k)] = 0.0  # set initial value to 0, for Q(s,a)\n",
    "        \n",
    "        self.new_action_values = []\n",
    "    \n",
    "    #Selects an action \n",
    "    def choose_action(self, current_state):\n",
    "        # choose action according to policy eps-greedy\n",
    "        if np.random.uniform(0, 1) <= self.eps:\n",
    "            action = np.random.choice(self.actions)\n",
    "            if DEBUG:\n",
    "                print(\"selecting random action\")\n",
    "        else:\n",
    "            action = self.best_action(current_state)\n",
    "        return action\n",
    "\n",
    "    #executes the selected action and updates the current state of the agent\n",
    "    def take_action(self, action):\n",
    "        position = self.State.nxt_position(action)\n",
    "        self.State.state = position\n",
    "\n",
    "    #Shows the best action for a state based on Q-Learning method    \n",
    "    def best_action(self, state):\n",
    "        best = -1\n",
    "        max_val = -100000000\n",
    "        for a in self.actions:\n",
    "            q_val = self.action_values[state[0], state[1], a]\n",
    "            if q_val >= max_val:\n",
    "                max_val = q_val\n",
    "                best = a\n",
    "        return best\n",
    "\n",
    "    #Calculates max q-value for a state\n",
    "    def q_max(self, state):\n",
    "        best = self.best_action(state)\n",
    "        return self.action_values[state[0], state[1], best]\n",
    "\n",
    "    #Implements q-learning\n",
    "    def q_learning(self, episodes):       \n",
    "        # Q-learning implementation\n",
    "        x = 0  \n",
    "        while x < episodes:\n",
    "            # Init S\n",
    "            self.State.isEnd = False\n",
    "            self.State.state = START  # Re init S Start state\n",
    "            step = 0\n",
    "            total_reward = 0  #Total reward per episode\n",
    "\n",
    "            if DEBUG:\n",
    "                print(\"**** Beginning episode\", x, \"****\")\n",
    "                self.show_values()\n",
    "\n",
    "            while True:  # repeat for each step of the episode (until S is terminal)\n",
    "\n",
    "                # Store current state for Q update\n",
    "                current_state = (self.State.state[0], self.State.state[1])\n",
    "\n",
    "                # Choose action A from S using policy derived from Q (e-greedy)\n",
    "                action = self.choose_action(current_state)\n",
    "\n",
    "                # Take action A observe R and next State S'\n",
    "                self.take_action(action)\n",
    "                reward = self.State.get_reward()\n",
    "                total_reward += reward  # Accumulate reward for this episode\n",
    "                self.State.is_end_func()\n",
    "                next_state = self.State.state[0], self.State.state[1]\n",
    "\n",
    "                # Update state action values\n",
    "                old_q = self.action_values[current_state[0], current_state[1], action]\n",
    "                max_q = self.q_max(next_state)\n",
    "                new_q = old_q + self.alpha * (reward + self.gamma * max_q - old_q)\n",
    "                self.action_values[current_state[0], current_state[1], action] = new_q\n",
    "\n",
    "                step += 1\n",
    "                if DEBUG:\n",
    "                    print(\"step\", step, \"state\", current_state, \"action\", action, \"reward\", reward,\n",
    "                          \"next_state\", next_state, \"old_q\", old_q, \"max_q\", max_q, \"new_q\", new_q)\n",
    "\n",
    "                # Check if s is terminal\n",
    "                if self.State.isEnd:\n",
    "                    self.episode_rewards.append(total_reward)  #Append total reward for episode\n",
    "                    break\n",
    "\n",
    "                # S <- S' automatically when I took the action                    \n",
    "\n",
    "            x += 1           \n",
    "            \n",
    "            # Decay epsilon\n",
    "            self.eps = max(self.min_eps, self.eps - self.eps_decay)  #Decrease epsilon\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            #Print out final epsilon values, uncomment the below print statement \n",
    "            #=====================================================================================================\n",
    "            #print(\"Final Epsilon Value:\", self.eps)      #NOTE - final value seems to be around 9.381755897326649e-14 == 0.00000000000009\n",
    "            #=====================================================================================================\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    #Displays the actions in a square format\n",
    "    def show_values(self):\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('-----------------------------------------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                mx_nxt_value = -1000\n",
    "                for a in self.actions:\n",
    "                    nxt_value = self.action_values[(i, j, a)]\n",
    "                    if nxt_value >= mx_nxt_value:\n",
    "                        mx_nxt_value = nxt_value                \n",
    "                out += str(round(mx_nxt_value, 3)).ljust(6) + ' | '\n",
    "            print(out)\n",
    "        print('-----------------------------------------------')\n",
    "    \n",
    "    \n",
    "    #Outputs the max action value estimate for each state to a (.txt) file\n",
    "    def max_action_value_output(self, filename):\n",
    "            with open(filename, 'w') as f:\n",
    "                for i in range(BOARD_ROWS):\n",
    "                    for j in range(BOARD_COLS):\n",
    "                        max_value = -float('inf')\n",
    "                        for a in self.actions:\n",
    "                            current_value = self.action_values[(i, j, a)]\n",
    "                            if current_value > max_value:\n",
    "                                max_value = current_value\n",
    "                        f.write(f\"State ({i}, {j}): {max_value}\\n\")    \n",
    "                        \n",
    "                        \n",
    "   \n",
    "\n",
    "                        \n",
    "    \n",
    "    def plot_rewards_per_episode(self):\n",
    "        episode_numbers = range(1, len(self.episode_rewards) + 1)\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.plot(episode_numbers, self.episode_rewards, color='black')\n",
    "        max_reward = max(self.episode_rewards)\n",
    "        plt.axhline(y=max_reward, color='red', linewidth=3, linestyle='--', label=f'Max Reward: {max_reward}')\n",
    "        avg_reward = self.average_reward()\n",
    "        plt.axhline(y=avg_reward, color='green', linewidth=3, linestyle='--', label=f'Avg Reward: {avg_reward}')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.title('Reward per Episode')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def average_reward(self):\n",
    "        return np.mean(self.episode_rewards) if self.episode_rewards else 0.0\n",
    "        \n",
    "        \n",
    "#Creates an agent, implements q-learning into it for a specified number of episodes, shows the\n",
    "#results and outputs max action values to .txt file       \n",
    "if __name__ == \"__main__\":\n",
    "    ag = Agent()\n",
    "    ag.q_learning(10000)\n",
    "    ag.show_values()\n",
    "    ag.max_action_value_output('C:/Users/Cathal/OneDrive/Masters/2nd Year/Sem 2/Agents, Multi-Agent Systems and Reinforcement Learning/Assignments/Assignment 2/Max Action Values (.txt)/Epsilon_Decay.txt')  #Save values to file\n",
    "    ag.plot_rewards_per_episode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b6de5b",
   "metadata": {},
   "source": [
    "REFERENCES:\n",
    "- Lecture Notes\n",
    "- Matplotlib Documentation - https://matplotlib.org/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd3ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART2 - Improved Parameters (Based on the first impplementation, just with adjusted hyperparamaters)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#Variables\n",
    "BOARD_ROWS = 5 #Number of rows \n",
    "BOARD_COLS = 5 #Number of columns\n",
    "WIN_STATE = (4, 4) #Position of the win state - bottom right corner\n",
    "HOLES = [(1, 0), (1, 3), (3, 1), (4, 2)] #Position of the holes\n",
    "START = (0, 0) #Starting position of the agent\n",
    "\n",
    "DEBUG = False  # set to true to enable verbose output\n",
    "\n",
    "\n",
    "class State:\n",
    "    def __init__(self, state=START):        \n",
    "        self.state = state\n",
    "        self.isEnd = False        \n",
    "\n",
    "    #Returns a reward of 10 if end state is reaches\n",
    "    #A reward of -5 if a hole is fallen into\n",
    "    #Or a reward of -1 if any other movement besides those \n",
    "    def get_reward(self):\n",
    "        if self.state == WIN_STATE:\n",
    "            return 10.0        \n",
    "        elif self.state in HOLES:\n",
    "            return -5.0\n",
    "        else:\n",
    "            return -1.0\n",
    "\n",
    "        \n",
    "    #Checks if an end state is reached - either falling into a hold or reaching the end state, if so, game is ended   \n",
    "    def is_end_func(self):\n",
    "        if (self.state == WIN_STATE) or (self.state in HOLES):\n",
    "            self.isEnd = True\n",
    "\n",
    "            \n",
    "    #Responsible for moving the agent in one of 4 directions - up,down,left or right\n",
    "    def nxt_position(self, action):\n",
    "        if action == 0:                \n",
    "            nxt_state = (self.state[0] - 1, self.state[1]) #0 - moves agent up\n",
    "        elif action == 1:\n",
    "            nxt_state = (self.state[0] + 1, self.state[1]) #1 - moves agent down\n",
    "        elif action == 2:\n",
    "            nxt_state = (self.state[0], self.state[1] - 1) #2 - moves agent left\n",
    "        else:\n",
    "            nxt_state = (self.state[0], self.state[1] + 1) #3 - moves agent right\n",
    "            \n",
    "        #Makes sure that the movement is valid within the square, if its not, agent remains in the same position      \n",
    "        if (nxt_state[0] >= 0) and (nxt_state[0] < BOARD_ROWS):\n",
    "            if (nxt_state[1] >= 0) and (nxt_state[1] < BOARD_COLS):\n",
    "                return nxt_state  \n",
    "        return self.state  \n",
    "\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = [0, 1, 2, 3]  # up, down, left, right\n",
    "        self.State = State()\n",
    "        self.gamma = 0.6\n",
    "        self.alpha = 0.75\n",
    "        self.eps = 0.05\n",
    "        self.episode_rewards = []  \n",
    "        \n",
    "        # initialise state values\n",
    "        self.action_values = {}        \n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                for k in range(len(self.actions)):\n",
    "                    self.action_values[(i, j, k)] = 0.0  # set initial value to 0, for Q(s,a)\n",
    "        \n",
    "        self.new_action_values = []\n",
    "    \n",
    "    #Selects an action \n",
    "    def choose_action(self, current_state):\n",
    "        # choose action according to policy eps-greedy\n",
    "        if np.random.uniform(0, 1) <= self.eps:\n",
    "            action = np.random.choice(self.actions)\n",
    "            if DEBUG:\n",
    "                print(\"selecting random action\")\n",
    "        else:\n",
    "            action = self.best_action(current_state)\n",
    "        return action\n",
    "\n",
    "    #executes the selected action and updates the current state of the agent\n",
    "    def take_action(self, action):\n",
    "        position = self.State.nxt_position(action)\n",
    "        self.State.state = position\n",
    "\n",
    "    #Shows the best action for a state based on Q-Learning method    \n",
    "    def best_action(self, state):\n",
    "        best = -1\n",
    "        max_val = -100000000\n",
    "        for a in self.actions:\n",
    "            q_val = self.action_values[state[0], state[1], a]\n",
    "            if q_val >= max_val:\n",
    "                max_val = q_val\n",
    "                best = a\n",
    "        return best\n",
    "\n",
    "    #Calculates max q-value for a state\n",
    "    def q_max(self, state):\n",
    "        best = self.best_action(state)\n",
    "        return self.action_values[state[0], state[1], best]\n",
    "\n",
    "    #Implements q-learning\n",
    "    def q_learning(self, episodes):       \n",
    "        # Q-learning implementation\n",
    "        x = 0  # episode counter\n",
    "        while x < episodes:\n",
    "            # Init S\n",
    "            self.State.isEnd = False\n",
    "            self.State.state = START  # Re init S Start state\n",
    "            step = 0\n",
    "            total_reward = 0  #Total reward per episode\n",
    "\n",
    "            if DEBUG:\n",
    "                print(\"**** Beginning episode\", x, \"****\")\n",
    "                self.show_values()\n",
    "\n",
    "            while True:  # repeat for each step of the episode (until S is terminal)\n",
    "\n",
    "                # Store current state for Q update\n",
    "                current_state = (self.State.state[0], self.State.state[1])\n",
    "\n",
    "                # Choose action A from S using policy derived from Q (e-greedy)\n",
    "                action = self.choose_action(current_state)\n",
    "\n",
    "                # Take action A observe R and next State S'\n",
    "                self.take_action(action)\n",
    "                reward = self.State.get_reward()\n",
    "                total_reward += reward  #Accumulate reward for episode\n",
    "                self.State.is_end_func()\n",
    "                next_state = self.State.state[0], self.State.state[1]\n",
    "\n",
    "                # Update state action values\n",
    "                old_q = self.action_values[current_state[0], current_state[1], action]\n",
    "                max_q = self.q_max(next_state)\n",
    "                new_q = old_q + self.alpha * (reward + self.gamma * max_q - old_q)\n",
    "                self.action_values[current_state[0], current_state[1], action] = new_q\n",
    "\n",
    "                step += 1\n",
    "                if DEBUG:\n",
    "                    print(\"step\", step, \"state\", current_state, \"action\", action, \"reward\", reward,\n",
    "                          \"next_state\", next_state, \"old_q\", old_q, \"max_q\", max_q, \"new_q\", new_q)\n",
    "\n",
    "                # Check if s is terminal\n",
    "                if self.State.isEnd:\n",
    "                    self.episode_rewards.append(total_reward)  #Append total reward for episode\n",
    "                    break\n",
    "\n",
    "                # S <- S' automatically when I took the action                    \n",
    "\n",
    "            x += 1           \n",
    "            \n",
    "    #Displays the actions in a square format\n",
    "    def show_values(self):\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('-----------------------------------------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                mx_nxt_value = -1000\n",
    "                for a in self.actions:\n",
    "                    nxt_value = self.action_values[(i, j, a)]\n",
    "                    if nxt_value >= mx_nxt_value:\n",
    "                        mx_nxt_value = nxt_value                \n",
    "                out += str(round(mx_nxt_value, 3)).ljust(6) + ' | '\n",
    "            print(out)\n",
    "        print('-----------------------------------------------')\n",
    "    \n",
    "    \n",
    "    #Outputs the max action value estimate for each state to a (.txt) file\n",
    "    def max_action_value_output(self, filename):\n",
    "            with open(filename, 'w') as f:\n",
    "                for i in range(BOARD_ROWS):\n",
    "                    for j in range(BOARD_COLS):\n",
    "                        max_value = -float('inf')\n",
    "                        for a in self.actions:\n",
    "                            current_value = self.action_values[(i, j, a)]\n",
    "                            if current_value > max_value:\n",
    "                                max_value = current_value\n",
    "                        f.write(f\"State ({i}, {j}): {max_value}\\n\")    \n",
    "                        \n",
    "                        \n",
    "   \n",
    "\n",
    "                        \n",
    "    \n",
    "    def plot_rewards_per_episode(self):\n",
    "        episode_numbers = range(1, len(self.episode_rewards) + 1)\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.plot(episode_numbers, self.episode_rewards, color='black')\n",
    "        max_reward = max(self.episode_rewards)\n",
    "        plt.axhline(y=max_reward, color='red', linewidth=3, linestyle='--', label=f'Max Reward: {max_reward}')\n",
    "        avg_reward = self.average_reward()\n",
    "        plt.axhline(y=avg_reward, color='green', linewidth=3, linestyle='--', label=f'Avg Reward: {avg_reward}')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.title('Reward per Episode')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "      \n",
    "        \n",
    "    def average_reward(self):\n",
    "        return np.mean(self.episode_rewards) if self.episode_rewards else 0.0\n",
    "        \n",
    "         \n",
    "        \n",
    "#Creates an agent, implements q-learning into it for a specified number of episodes, shows the\n",
    "#results and outputs max action values to .txt file       \n",
    "if __name__ == \"__main__\":\n",
    "    ag = Agent()\n",
    "    ag.q_learning(10000)\n",
    "    ag.show_values()\n",
    "    ag.max_action_value_output('C:/Users/Cathal/OneDrive/Masters/2nd Year/Sem 2/Agents, Multi-Agent Systems and Reinforcement Learning/Assignments/Assignment 2/Max Action Values (.txt)/Improved_Parameters.txt')  #Save values to file\n",
    "    ag.plot_rewards_per_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926f7a23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
